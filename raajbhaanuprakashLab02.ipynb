{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "raajbhaanuprakashLab02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raajprakash/Lab_Exercises/blob/master/raajbhaanuprakashLab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKlL7xym7mgd",
        "colab_type": "text"
      },
      "source": [
        "# <center> LAB 2: Multiple Regression, Regularised Regression and Logistic Regression<br> <small>RÃ©da DEHAK<br> 17 June 2019</small> </center>\n",
        "\n",
        "The goal of this lab is :\n",
        "    - Fit generalised linear models with ridge or Lasso regularisations\n",
        "    - Test the logistic regression on classification problems\n",
        "    - Send your final notebook by email at dsa@dehak.org before next monday\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeuYA6Ez7mge",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Regularised Regression \n",
        "### Import Data\n",
        "\n",
        "The following dataset is from Hastie, Tibshirani and Friedman (2009), from a study by Stamey et al. (1989) of prostate cancer, measuring the correlation between the level of a prostate-specific antigen and some covariates. The covariates are\n",
        "- lcavol : log-cancer volume\n",
        "-  lweight : log-prostate weight\n",
        "-  age : age of patient\n",
        "-  lbhp : log-amount of benign hyperplasia\n",
        "-  svi : seminal vesicle invasion\n",
        "-  lcp : log-capsular penetration\n",
        "-  gleason : Gleason Score,\n",
        "-  lpsa is the response variable, log-psa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXBC0ivW7mgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "f7c89a40-f83b-49e2-e859-e4d02aefdafd"
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Lab_Exercises'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OBeoreG7mgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "68853b61-bbc3-4df4-db5f-253d4dac6b0b"
      },
      "source": [
        "fin = open('data2.pkl', 'rb')\n",
        "xtrain = pickle.load(fin)\n",
        "ytrain = pickle.load(fin)\n",
        "Xtest = pickle.load(fin)\n",
        "Ytest = pickle.load(fin)\n",
        "fin.close()\n",
        "\n",
        "print('Train data : ', xtrain.shape, ' ', ytrain.shape)\n",
        "print('Test data : ', Xtest.shape, ' ', Ytest.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-69ab154b9a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data2.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mYtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data2.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HnwNYJ_7mgp",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "Using the program of TP 1, compute the linear regression weight $w$\n",
        "\n",
        "$$y = g(x) = W^T x =\\sum_{d=0}^7 w_d x_d$$\n",
        "with $x_0 = 1$\n",
        "\n",
        "The linear regression consists in finding the parameters $W$ which minimizes the \n",
        "quadratic error:\n",
        "$$E(W) = \\frac{1}{60}\\sum_{i=1}^{60}\\left(g(x_i) - y_i\\right)^2$$\n",
        "\n",
        "The vector $W$ which minimize $E(W)$ is defined as follow:\n",
        "$$W = (X X^T)^{-1}X Y$$\n",
        "\n",
        "Compute the vector $W$ wich minimize $E(W)$ :\n",
        "- Compute $w$ using the exact solution\n",
        "- Compute the error on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7nIMiZE7mgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X =np.vstack((np.ones(xtrain.shape[1]),xtrain))\n",
        "E =np.linalg.inv(X @ X.T) @ X @ ytrain\n",
        "print(E)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P9N1cZE7mgs",
        "colab_type": "text"
      },
      "source": [
        "- Check that you obtain the same $W$ with sklean.linear_model.LinearRegression?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP3tIKDW7mgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg_lin = LinearRegression().fit(xtrain.T, ytrain.reshape((60,1)))\n",
        "print(\"A = \", reg_lin.coef_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVUGn12j7mgv",
        "colab_type": "text"
      },
      "source": [
        "### Ridge regression\n",
        "\n",
        "The ridge regression consists in finding the parameters $W$ which minimizes:\n",
        "$$\\frac{1}{60}\\sum_{i=1}^{60}\\left(W^T x_i - y_i\\right)^2 + \\alpha \\|W\\|_2^2$$ \n",
        "\n",
        "- Using linear_model.Ridge and $\\alpha = 0.$, check that you obtain the same $W$ as linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6-5mEIF7mgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ridge_reg = linear_model.Ridge(alpha = 0.)\n",
        "\n",
        "ridge_reg.fit(xtrain.T, ytrain)\n",
        "print('E =', ridge_reg.coef_)\n",
        "f = np.mean((ridge_reg.predict(Xtest.T) - Ytest) ** 2)\n",
        "print('Erreur = ', f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s257VTBM7mgz",
        "colab_type": "text"
      },
      "source": [
        "In this part, we will check the influence of $\\alpha$ on the solution of the linear regression\n",
        "\n",
        "- Train a ridge regression with different values of $\\alpha$ = np.logspace(-5, 5, 200), save the values of W, Mean squared errors on train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h7vLIPb7mg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphas = np.logspace(-5, 5, 200)\n",
        "e=[]\n",
        "w=[]\n",
        "for a in alphas:\n",
        "    rr = linear_model.Ridge(alpha=a)\n",
        "    rr.fit(xtrain.T, ytrain)\n",
        "    yp = rr.predict(xtrain.T)\n",
        "    e1 = np.mean((ytrain - yp)**2)\n",
        "    ytp = rr.predict(Xtest.T)\n",
        "    e2 = np.mean((Ytest - ytp)**2)\n",
        "    e.append(np.array([e1, e2]))\n",
        "    w.append(rr.coef_)\n",
        "\n",
        "e=np.array(e).T\n",
        "w=np.array(w).T\n",
        "print(e.shape)\n",
        "print(w.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSSvsQI47mg6",
        "colab_type": "text"
      },
      "source": [
        "- Plot how evolve each $W_i$ through the sequence of $\\alpha$ values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhChQHhR7mg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(7):\n",
        "    plt.semilogx(alphas, w[i, :])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysXcXG2g7mg-",
        "colab_type": "text"
      },
      "source": [
        "- Plot how evolve the mean square error on train and test data through the sequence of $\\alpha$ values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AygZlOIE7mg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Mean Squared Error on train data\n",
        "plt.semilogx(alphas, e[0, :], color = 'blue')\n",
        "# Plot Mean Squared Error on train data\n",
        "plt.semilogx(alphas, e[1, :], color = 'red')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na8nnSbW7mhD",
        "colab_type": "text"
      },
      "source": [
        "- Conclude? (Which is the best value for $\\alpha$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vn2N1mp7mhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(2):\n",
        "    plt.semilogx(alphas, e[i, :])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DtbbPbB7mhI",
        "colab_type": "text"
      },
      "source": [
        "### Lasso regression\n",
        "\n",
        "The ridge regression consists in finding the parameters $W$ which minimizes:\n",
        "$$\\frac{1}{2 \\times 60}\\sum_{i=1}^{60}\\left(W^T x_i - y_i\\right)^2 + \\alpha \\|W\\|_1$$\n",
        "\n",
        "- Using linear_model.Lasso and $\\alpha = 0.$, check that you obtain the same $W$ as linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d-U19l27mhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQB-ug_-7mhN",
        "colab_type": "text"
      },
      "source": [
        "In this part, we will check the influence of $\\alpha$ on the solution of the linear regression\n",
        "\n",
        "- Train a Lasso regression with different values of $\\alpha$ = np.logspace(-5, 5, 200), save the values of W, Mean squared errors on train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwWkoHAj7mhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphas = np.logspace(-5, 5, 200)\n",
        "e=[]\n",
        "w=[]\n",
        "for a in alphas:\n",
        "    ...\n",
        "    etrain = ...\n",
        "    etest = ...\n",
        "    e.append(np.array([etrain, etest]))\n",
        "    w.append(...)\n",
        "\n",
        "e=np.array(e).T\n",
        "w=np.array(w).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYz1_w3D7mhR",
        "colab_type": "text"
      },
      "source": [
        "- Plot how evolve each $W_i$ through the sequence of $\\alpha$ values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4TBBjvD7mhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(7):\n",
        "    plt.semilogx(alphas, w[i, :])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi97uMlt7mhV",
        "colab_type": "text"
      },
      "source": [
        "- Plot how evolve the mean square error on train and test data through the sequence of $\\alpha$ values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyMPjzIW7mhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Mean Squared Error on train data\n",
        "plt.semilogx(alphas, e[0, :], color = 'blue')\n",
        "# Plot Mean Squared Error on train data\n",
        "plt.semilogx(alphas, e[1, :], color = 'red')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0N_aM87mhb",
        "colab_type": "text"
      },
      "source": [
        "- Conclude? (Which is the best value for $\\alpha$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5eQN2cd7mhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH7A8Md47mhe",
        "colab_type": "text"
      },
      "source": [
        "Compare the result with ridge solution?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB_XP8Vf7mhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ccPt0PN7mhh",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Logistic Regression \n",
        "### Import Data\n",
        "\n",
        "We will use the Wine dataset from UCI. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of thirteen constituents found in each of the three types of wines.\n",
        "\n",
        "# Loading and Plotting Data\n",
        " \n",
        "First, we will use only two features from the data set: alcohol and ash (We can plot the solution in 2D space). The labels are supplied as an array of data with values from 1 to 3, but at first, we want a simple binary regression problem with a yes or no answer.  \n",
        "\n",
        "We filter the data set, reducing it to only include wines with labels 1 or 2.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kw6oo9Z7mhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "data = pd.read_csv('data.txt')\n",
        "\n",
        "reduced = data[data['class'] <= 2]\n",
        "X = reduced[['alcohol', 'ash']].to_numpy()\n",
        "y = label_binarize(reduced['class'].to_numpy(), [1, 2])[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZSNIedW7mho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
        "print('train:', len(Xtrain), 'test:', len(Xtest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGByEkW97mhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SCRON = ['+', 'x', '.']\n",
        "COLOER = ['red', 'green', 'blue']\n",
        "\n",
        "def plot_points(xy, labels):\n",
        "    \n",
        "    for i, label in enumerate(set(labels)):\n",
        "        points = np.array([xy[j,:] for j in range(len(xy)) if labels[j] == label])\n",
        "        marker = SCRON[i % len(SCRON)]\n",
        "        color = COLOER[i % len(COLOER)]\n",
        "        plt.scatter(points[:,0], points[:,1], marker=marker, color=color)\n",
        "\n",
        "plot_points(Xtrain, ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiZWOSY37mht",
        "colab_type": "text"
      },
      "source": [
        "We can see that we can plot line that could divide the two colored points with a small amount of error.\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "To implement logistic regression, we need to define the cost function $J(\\theta)$, and compute the partial derivatives of $J(\\theta)$. As we have seen previously:\n",
        "\n",
        "$$\n",
        "J(\\theta) =-\\frac{1}{N}\\sum_{i=1}^{N}y^{i}\\log(f_\\theta(x^{i}))+(1-y^{i})\\log(1-f_\\theta(x^{i}))\n",
        "$$\n",
        "\n",
        "where $f_\\theta(x)$ is the logistic function\n",
        "\n",
        "$$\n",
        "f_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
        "$$\n",
        "\n",
        "- Compute the partiel derivatives of $J(\\theta)$ and write the two functions:\n",
        "    - cost(theta, X, y) which compute the value of $J(\\theta)$\n",
        "    - gradient(theta, X, y) which compute the value of the gradient of $J(\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SunPiBuY7mhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(theta, X):\n",
        "    return 1 / (1 + np.exp(-X @ theta))\n",
        "\n",
        "def cost(theta, X, y):\n",
        "    theta = theta[:,None]\n",
        "    y = y[:,None]\n",
        "    \n",
        "    hyp = sigmoid(theta, X)\n",
        "    pos = np.multiply(y, np.log(hyp))\n",
        "    neg = np.multiply((1 - y), np.log(1 - hyp))\n",
        "    \n",
        "    return -np.sum(pos + neg) / (len(X))\n",
        "\n",
        "def gradient(theta, X, y):\n",
        "    theta = theta[:,None]\n",
        "    y = y[:,None]\n",
        "    \n",
        "    error = sigmoid(theta, X) - y\n",
        "    return X.T.dot(error) / len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s2cSTYk7mhy",
        "colab_type": "text"
      },
      "source": [
        "- Using the function scipy.optimize.fmin_tnc which performs a gradient descent algorithm, write a function Train(x, y) which compute $\\theta$ that minimize $J(\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDWarEpy7mhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(X, y):\n",
        "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    result = fmin_tnc(func=cost, x0=theta, fprime=gradient, disp=5, args=(X, y))\n",
        "    print(result)\n",
        "    return result[0]\n",
        "\n",
        "W = train(Xtrain, ytrain)\n",
        "print('w = ', W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfAvJ73H7mh1",
        "colab_type": "text"
      },
      "source": [
        "- compute the value of the best $\\theta$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbXX5_TM7mh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "def predict(theta, X):\n",
        "    R = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    return (sigmoid(theta, R) >= 0.5).astype(int)\n",
        "\n",
        "def plot_boundary(X, pred):\n",
        "    \n",
        "    x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
        "    y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
        "    \n",
        "    xs, ys = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 200),\n",
        "        np.linspace(y_min, y_max, 200)\n",
        "    )\n",
        "\n",
        "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
        "    zs = pred(xys).reshape(xs.shape)\n",
        "    print(zs)\n",
        "    plt.contour(xs, ys, zs, colors='black')\n",
        "  \n",
        "plot_boundary(Xtrain, lambda x: predict(W, x))\n",
        "plot_points(Xtrain, ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut6ELW9k7mh6",
        "colab_type": "text"
      },
      "source": [
        "- Plot the boundary and checks that it is linear?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB8jla3F7mh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "def predict(theta, X):\n",
        "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
        "    return (sigmoid(theta, X) >= 0.5).astype(int)\n",
        "\n",
        "def plot_boundary(X, pred):\n",
        "    \n",
        "    x_min, x_max = X[:,0].min() - .1, X[:,0].max() + .1\n",
        "    y_min, y_max = X[:,1].min() - .1, X[:,1].max() + .1\n",
        "    \n",
        "    xs, ys = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 200),\n",
        "        np.linspace(y_min, y_max, 200)\n",
        "    )\n",
        "\n",
        "    xys = np.column_stack([xs.ravel(), ys.ravel()])\n",
        "    zs = pred(xys).reshape(xs.shape)\n",
        "    plt.contour(xs, ys, zs, colors='black')\n",
        "  \n",
        "plot_boundary(Xtrain, lambda x: predict(W, x))\n",
        "plot_points(Xtrain, ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_iq-eix7mh8",
        "colab_type": "text"
      },
      "source": [
        "- Using sklearn.metrics, compute the accuracy, the precision and the recall of this classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyEIlM4n7mh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "predictions = predict(W, Xtest)\n",
        "print('accuracy:', accuracy_score(ytest, predictions))\n",
        "print('precision:', precision_score(ytest, predictions, average='macro'))\n",
        "print('recall:', recall_score(ytest, predictions, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ulXoF1C7mh_",
        "colab_type": "text"
      },
      "source": [
        "- How can we obtain a quadratic boundary? check it and plot the boundary?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rU-gD0F7miA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def transform(x):\n",
        "    #return PolynomialFeatures(2).fit_transform(x)\n",
        "    return np.vstack((...))\n",
        "\n",
        "W = train(transform(Xtrain), ytrain)\n",
        "print(W)\n",
        "plot_points(Xtrain, ytrain)\n",
        "plot_boundary(Xtrain, lambda x: ...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCh_NfWg7miC",
        "colab_type": "text"
      },
      "source": [
        "- Compute the accuracy, the precision and the recall of this classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoY93LiA7miD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = predict(W, transform(Xtest))\n",
        "print('accuracy:', accuracy_score(ytest, predictions))\n",
        "print('precision:', precision_score(ytest, predictions, average='macro'))\n",
        "print('recall:', recall_score(ytest, predictions, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMgDrxpA7miG",
        "colab_type": "text"
      },
      "source": [
        "# Multinomial Logistic Regression\n",
        "\n",
        "The next step is something more interesting: we use a similar set of two features from the data set (this time alcohol and flavanoids), but with all three labels instead of two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMbYFZP7miG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data[['alcohol', 'flavanoids']].to_numpy()\n",
        "y = data[['class']].to_numpy()\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
        "ytrain = label_binarize(ytrain, [1, 2, 3])\n",
        "plot_points(Xtrain, ytrain.argmax(axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gp3G4XV7miI",
        "colab_type": "text"
      },
      "source": [
        "The plotted data points again suggest some obvious linear boundaries between the three classes.\n",
        "\n",
        "We can solve this problem as three one-vs-all problems, and re-use all the previous code. In this part, we will try another solution inspired from softmax function known as softmax regression (See C.Bishop, \"Pattern Recognition and Machine Learning\", 2006, Springer).\n",
        "\n",
        "$$\n",
        "SoftMax_\\Theta(x, k) = \\frac{e^{\\theta_k^Tx}}{\\sum\\limits_{c=1}^K e^{\\theta_c^Tx}}\n",
        "$$\n",
        "\n",
        "The cost function is defined as follows:\n",
        "\n",
        "$$\n",
        "J(\\Theta) =-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^3\\left[y_k^{i}\\log\\left(SoftMax_\\Theta\\left(x^{i}, k\\right)\\right)\\right]\n",
        "$$\n",
        "\n",
        "- Propose a solution using the SoftMax function and test it with linear and quadratic separator? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDoI9zr17miJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(w, x):\n",
        "    w = w.reshape((x.shape[1], -1))\n",
        "    K = w.shape[1]\n",
        "    N = x.shape[0]\n",
        "    y = np.zeros((N, K))\n",
        "    for k in range(K):\n",
        "        y[:, k] = np.exp(x.dot(w[:, k]))\n",
        "    s = np.sum(y, axis = 1)\n",
        "    y = np.diag(1./s) @ y\n",
        "    return y\n",
        "\n",
        "def cost(w, x, y):\n",
        "    w = w.reshape((x.shape[1], -1))\n",
        "    yhat = softmax(w, x)\n",
        "    K = w.shape[1]\n",
        "    s = 0\n",
        "    for k in range(K):\n",
        "        s += np.sum(y[:, k] * np.log(yhat[:, k])) \n",
        "    return -s / len(y) \n",
        "\n",
        "def grad(w, x, y):\n",
        "    yhat = softmax(w, x)\n",
        "    error = yhat - y\n",
        "    return x.T @ error / X.shape[0] \n",
        "\n",
        "def train(X, y):\n",
        "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
        "    theta = np.zeros((X.shape[1], y.shape[1]))\n",
        "    result = fmin_tnc(func=cost, x0=theta, fprime=grad, disp=5, args=(X, y))\n",
        "    \n",
        "    return result[0].reshape((X.shape[1], -1))\n",
        "\n",
        "W = train(Xtrain, ytrain)\n",
        "print(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7eodwR_7miL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_multi(x, w):\n",
        "    x = np.insert(x, 0, np.ones(len(x)), axis=1)\n",
        "    preds = softmax(w, x)\n",
        "    return preds.argmax(axis=1)\n",
        "\n",
        "predictions = predict_multi(Xtest, W) + 1\n",
        "\n",
        "print('accuracy:', accuracy_score(ytest, predictions))\n",
        "print('precision:', precision_score(ytest, predictions, average='macro'))\n",
        "print('recall:', recall_score(ytest, predictions, average='macro'))\n",
        "\n",
        "plot_points(Xtrain, ytrain.argmax(axis=1))\n",
        "plot_boundary(Xtrain, lambda x: predict_multi(x, W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEtbgw8O7miO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import sys\n",
        "from  numpy.matlib import repmat\n",
        "\n",
        "class MLP():\n",
        "    \n",
        "    \n",
        "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
        "            n_neurons     : number of neurones.\n",
        "            l2            : lambda value for L2-regularization.\n",
        "            epochs        : number of passes over the training set.\n",
        "            learning_rate : learning rate.\n",
        "            batch_size    : number of training samples per minibatch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, d_input, d_out, n_neurons=30,\n",
        "                 l2=0., epochs=100, learning_rate=0.001, batch_size=1):\n",
        "        self.input = d_input\n",
        "        self.out = d_out\n",
        "        self.n_neurons = n_neurons\n",
        "        self.l2 = l2\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        ########################\n",
        "        # Weight initialization\n",
        "        ########################\n",
        "\n",
        "        # weights for input -> hidden\n",
        "        self.w1 = np.random.normal(loc=0.0, scale=0.1,\n",
        "                                      size=(self.input + 1, self.n_neurons))\n",
        "\n",
        "        # weights for hidden -> output\n",
        "\n",
        "        self.w2 = np.random.normal(loc=0.0, scale=0.1,\n",
        "                                        size=(self.n_neurons + 1, self.out))\n",
        "\n",
        "        self.eval= {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
        "\n",
        "        \n",
        "    def xlogy(self, x, y):\n",
        "        ind = np.nonzero(x)\n",
        "        ret = np.zeros(y.shape)\n",
        "        ret[ind] = np.log(y[ind])\n",
        "        return ret\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
        "        return 1. / (1. + np.exp(-x))\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x)\n",
        "        return (exps / repmat(np.sum(exps, axis = 1).reshape((-1,1)), 1, exps.shape[1]))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Compute forward propagation step\"\"\"\n",
        "\n",
        "        # step 1: net input of hidden layer\n",
        "        # [n_samples, num_pixels] dot [num_pixels, n_neurons]\n",
        "        # -> [n_samples, n_neurons]\n",
        "        X = np.insert(X, 0, np.ones(len(X)), axis = 1)\n",
        "        z1 = X @ self.w1\n",
        "\n",
        "        # step 2: activation of hidden layer\n",
        "        a1 = self.sigmoid(z1)\n",
        "\n",
        "        # step 3: net input of output layer\n",
        "        # [n_samples, n_neurons] dot [n_neurons, n_classlabels]\n",
        "        # -> [n_samples, n_classlabels]\n",
        "\n",
        "        z2 = np.insert(a1, 0, np.ones(len(a1)), axis =1) @ self.w2\n",
        "\n",
        "        # step 4: activation output layer\n",
        "        output = self.softmax(z2)\n",
        "\n",
        "        return z1, a1, z2, output\n",
        "\n",
        "    def compute_cost(self, y_enc, output):\n",
        "        \"\"\"Compute cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_enc : array, shape = (n_samples, n_labels)\n",
        "            one-hot encoded class labels.\n",
        "        output : array, shape = [n_samples, n_output_units]\n",
        "            Activation of the output layer (forward propagation)\n",
        "\n",
        "        Returns\n",
        "        ---------\n",
        "        cost : float\n",
        "            Regularized cost\n",
        "\n",
        "        \"\"\"\n",
        "        L2_term = (self.l2 *\n",
        "                   (np.sum(self.w1 ** 2.) +\n",
        "                    np.sum(self.w2 ** 2.)))\n",
        "\n",
        "        term = np.sum(self.xlogy(y_enc, output), axis = 1)\n",
        "        cost = -np.mean(term) + L2_term\n",
        "        \n",
        "        return cost\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"  Predict class labels \"\"\"\n",
        "        z1, a1, z2, output = self.forward(X)\n",
        "        y_pred = np.argmax(z2, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "    def fit(self, x_train, y_train, x_valid, y_valid):\n",
        "        \"\"\" Learn weights from training data \"\"\"\n",
        "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
        "        num_pixels = x_train.shape[1]\n",
        "        y_train_enc = to_categorical(y_train)\n",
        "        n_class=y_train_enc.shape[1]\n",
        "        \n",
        "        # iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            # iterate over minibatches\n",
        "            indices = np.arange(x_train.shape[0])\n",
        "\n",
        "            for start_idx in range(0, indices.shape[0] - self.batch_size +\n",
        "                                   1, self.batch_size):\n",
        "                batch_id = indices[start_idx:start_idx + self.batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                z1, a1, z2, output = self.forward(x_train[batch_id])\n",
        "                #print(y_train_enc[batch_id, :].shape)\n",
        "                c1 = self.compute_cost(y_train_enc[batch_id, :], output)\n",
        "                #print(c1)\n",
        "                ##################\n",
        "                # Backpropagation\n",
        "                ##################\n",
        "               \n",
        "                # [n_samples, n_classlabels]\n",
        "                #dlost_doutput = -y_train_enc[batch_id, :] / output\n",
        "                #dsoftmax = output * (1 - output)\n",
        "                delta2 = output - y_train_enc[batch_id, :]\n",
        "                #dlost_doutput * dsoftmax\n",
        "                \n",
        "                \n",
        "                dlogistic = a1 * (1-a1)\n",
        "                delta1 = (delta2 @ self.w2[1:,:].T) * dlogistic \n",
        "                gradient_w1 = np.insert(x_train[batch_id], 0, np.ones(len(x_train[batch_id])), axis =1).T @ delta1\n",
        "                gradient_w2 = np.insert(a1, 0, np.ones(len(a1)), axis =1).T @ delta2\n",
        "                \n",
        "                # Regularization and weight updates\n",
        "                delta_w1 = (gradient_w1 + self.l2 * np.insert(self.w1[1:,:], 0, np.zeros(self.w1.shape[1]), axis = 0))              \n",
        "                self.w1 -= self.learning_rate * delta_w1\n",
        "                \n",
        "                delta_w2 = (gradient_w2 + self.l2 * np.insert(self.w2[1:,:], 0, np.zeros(self.w2.shape[1]), axis = 0))\n",
        "                self.w2 -= self.learning_rate * delta_w2\n",
        "                \n",
        "                \n",
        "            #############\n",
        "            # Evaluation\n",
        "            #############\n",
        "\n",
        "            # Evaluation after each epoch during training\n",
        "            z1, a1, z2, output = self.forward(x_train)\n",
        "            \n",
        "            cost = self.compute_cost(y_enc=y_train_enc,\n",
        "                                      output=output)\n",
        "\n",
        "            y_train_pred = self.predict(x_train)\n",
        "            y_valid_pred = self.predict(x_valid)\n",
        "\n",
        "            train_acc = accuracy_score(y_train, y_train_pred)\n",
        "            #((np.sum(y_train == y_train_pred)).astype(np.float) /\n",
        "            #             x_train.shape[0])\n",
        "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n",
        "                        x_valid.shape[0])\n",
        "            sys.stderr.write('\\r%f %0*d/%d '\n",
        "                             '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
        "                             (cost, len(str(self.epochs)), i+1, self.epochs,\n",
        "                              train_acc*100, valid_acc*100))\n",
        "            sys.stderr.flush()\n",
        "\n",
        " \n",
        "                         \n",
        "            self.eval['cost'].append(cost)\n",
        "            self.eval['train_acc'].append(train_acc)\n",
        "            self.eval['valid_acc'].append(valid_acc)\n",
        "\n",
        "        return self\n",
        "\n",
        "        \n",
        "nn = MLP(2, 3, n_neurons=1000, \n",
        "                  l2=0.01, \n",
        "                  epochs=200, \n",
        "                  learning_rate=0.001,\n",
        "                  batch_size=1)\n",
        "\n",
        "nn.fit(Xtrain, Ytrain-1, Xtrain, Ytrain-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jb9EeNQ7miQ",
        "colab_type": "text"
      },
      "source": [
        "- Compute the accuracy, the precision and the recall of these classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8994Zo-7miR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "fdddb76f-7541-44f8-c029-a43174547635"
      },
      "source": [
        "plot_points(Xtrain, ytrain.argmax(axis=1))\n",
        "plot_boundary(Xtrain, lambda x: nn.predict(x))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2568f7cc6549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_points' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2r9gTJK7miT",
        "colab_type": "text"
      },
      "source": [
        "- Conclude?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl0OOlUo7miV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7e2s2ik7miW",
        "colab_type": "text"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "Next, we want to include all the features from the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE4I3tJ67miW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop('class', 1).to_numpy()\n",
        "y = data[['class']].to_numpy()\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
        "YTrain = label_binarize(ytrain, [1, 2, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvwZfnbj7miZ",
        "colab_type": "text"
      },
      "source": [
        "Because we are now significantly increasing the number of features, we apply regularisation  as part of new cost and gradient functions.  As we have seen with linear regression, regularization prevents overfitting, a situation where a large number of features allows the classifier to fit the training set *too* exactly, meaning that it fails to generalize well and perform accurately on data it hasn't yet seen.\n",
        "\n",
        "To avoid this problem, we add an additional term to the cost function\n",
        "\n",
        "$$\n",
        "J(\\theta) =-\\frac{1}{N}\\sum_{i=1}^{N}[y^{i}\\log(f_\\theta(x^{i}))+(1-y^{i})\\log(1-f_\\theta(x^{i}))] + \\frac{\\lambda}{2}\\|\\theta\\|_2^2\n",
        "$$\n",
        "\n",
        "- Compute the partiel derivatives of $J(\\theta)$ and define the update formula of the gradient descent algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfg3Ua1N_Ic7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(w, x, y, lamb):\n",
        "    w = w.reshape((x.shape[1], -1))\n",
        "    yhat = softmax(w, x)\n",
        "    K = w.shape[1]\n",
        "    s = 0\n",
        "    for k in range(K):\n",
        "        s += np.sum(y[:, k] * np.log(yhat[:, k])) + lamb / 2 * np.linalg.norm(w[:,k])**2\n",
        "    return -s / len(y) \n",
        "\n",
        "def grad(w, x, y, lamb):\n",
        "    w = w.reshape((x.shape[1], -1))\n",
        "    yhat = softmax(w, x)\n",
        "    error = yhat - y\n",
        "    return x.T @ error / X.shape[0] + lamb * w\n",
        "\n",
        "def train(X, y, lamb):\n",
        "    X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
        "    theta = np.zeros((X.shape[1], y.shape[1]))\n",
        "    result = fmin_tnc(func=cost, x0=theta, fprime=grad, disp=5, args=(X, y, lamb))\n",
        "    \n",
        "    return result[0].reshape((X.shape[1], -1))\n",
        "\n",
        "W = train(Xtrain, YTrain, 1.)\n",
        "print(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5osGzJN7mib",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br749-it7mib",
        "colab_type": "text"
      },
      "source": [
        "- Write a function that minimize $J(\\theta)$ and test it on the WINE dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OULuiiR7mic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lamb = np.linspace(0, 5, 200)\n",
        "etrain = []\n",
        "etest = []\n",
        "for i in range(200):\n",
        "    W = train(Xtrain, YTrain, lamb[i])\n",
        "    predictions = predict_multi(Xtrain, W) + 1\n",
        "    etrain.append(np.array([accuracy_score(ytrain, predictions), precision_score(ytrain, predictions, average='macro'), recall_score(ytrain, predictions, average='macro')]))\n",
        "    predictions = predict_multi(Xtest, W) + 1\n",
        "    etest.append(np.array([accuracy_score(ytest, predictions), precision_score(ytest, predictions, average='macro'), recall_score(ytest, predictions, average='macro')]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlpNL3KY7mid",
        "colab_type": "text"
      },
      "source": [
        "- Compare with non regularized version?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16xaRlIS7mid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "etrain=np.array(etrain).T\n",
        "etest=np.array(etest).T\n",
        "print(train(Xtrain, YTrain, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE4vjH-p7mig",
        "colab_type": "text"
      },
      "source": [
        "- Conclude?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxSSVeXI7mig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.semilogx(lamb, etrain[0, :], color='blue')\n",
        "plt.semilogx(lamb, etest[0, :], color='red')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}